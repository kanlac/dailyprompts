# 分布式共识

## 部署 3 个 K8S 节点，1 个控制平面节点，2 个工作节点，是否存在共识问题？

不存在，因为只有一个控制节点在对 etcd 进行写操作。

## 部署 3 个 K8S 节点，2 个控制平面节点，1 个工作节点，是否存在共识问题？

存在，且两个控制平面节点的共识数为 n/2+1 = 2，容错量为控制平面节点数 - 共识数 = 0，因此当其中一个故障，只剩下一个无法达成共识，此时集群将无法对 etcd 进行写操作，创建、更新、删除资源都会失败（读不受影响）。因此高可用要求至少 3 个控制平面节点。

## Patroni（Postgres 高可用方案）是否存在共识问题？

虽然一般来说基于 WAL log-shipping 的 PG 高可用方案都只会存在一个主节点，但还是可能存在共识问题——主节点挂掉的时候，如何选出下一个主？

而且共识能否达成不取决于 Postgres 节点数，而是一致性存储（etcd 或 ZK）节点数。只是有时它们一样。

在存在 3 个网络分区（数据中心）时，Patroni [要求](https://patroni.readthedocs.io/en/latest/ha_multi_dc.html) etcd 节点数至少有 3 个以实现高可用，总共一个 etcd 集群，容错率为 1。

如果只有两个数据中心，因为容错率为 0，Patroni 建议第二个分区只作为第一个分区的备份，做 2 个分开的 etcd 集群，并且当网络隔离发生时，由于第二个分区自己达不到共识数，它不能自动升级为主节点。这种时候只能手动关闭源集群或将源集群降级为备用节点，然后再手动升级第二个集群为主节点。

## 基于 K8s 的 Postgres 高可用方案（比如 CloudNativePG 或 Zalando-pg）是否存在共识问题？

只要它们使用的一致性存储（Etcd 集群）是奇数个节点，就不存在共识问题。换句话说，只要 K8s 有奇数个控制平面节点，就不用担心 PG 选举发生脑裂。因为 cnpg 和 zalando-pg 都是用 K8s operator 实现的，调度是由控制平面做，只要控制平面和 cnpg 的管理 Pod 能正常工作，就能完成选举。
